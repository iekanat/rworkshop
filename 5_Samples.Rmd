---
title: "Example Analysis"
author: "Irfan Kanat"
date: "11/04/2015"
output:
  pdf_document:
    fig_height: 3
    fig_width: 4
    keep_tex: yes
---


## Statistical Models

In this section, I will try to provide an introduction to using two simple statistical models in R: regression and logistic regression.

### Regression

If your dependent variable is continuous you can simply use regression.

For this demonstration, I will use the same Motor Trends dataset I used in Visualization section. 

```{r}
data(mtcars) # Get the data
?mtcars # Help on dataset
```

We will use lm() function to fit regular regression.

```{r}
?lm
```

Below I declare a model where I use horse power, cylinders, and transmission type to estimate gas milage. Pay attention to model specification:

```
mpg ~ hp + cyl + am
```

Here the left hand side of the tilde is the dependent variable. and the right hand side has all the predictors we use separated by plus signs.

```{r}
# Fit 
reg_0 <- lm( mpg ~ hp + cyl + am, data = mtcars) 
summary(reg_0)
```

Look at the R-squared value to see how much variance is explained by the model, the more the better.

You can access estimated values as follows. I used a head function to limit the output.

```{r}
head(reg_0$fitted.values)
```

You can use the fitted model to predict new datasets. Here I am modifying Datsun710 to see how the gas milage may have been influenced if the car was automatic instead of manual transmission.

```{r}
newCar <- mtcars[3,] # 3rd observation is Datsun 710
newCar$am <- 0 # What if it was automatic?
predict(reg_0, newdata = newCar) # Estimate went down by 4 miles
```

One way to see how your model did is to plot residuals. Ideally the residuals should be close to 0 and randomly distributed. If you see a pattern, it indicates misspecification.

```{r}
library(ggplot2)
# Plot the fitted values against real values
qplot(data=mtcars, x = mpg, y = reg_0$residuals) +
  stat_smooth(method = "lm", col = "red")

# Are the residuals normally distributed? 
shapiro.test(reg_0$residuals) # yes
```

Comparing models. If you are using the same dataset, and just adding or removing variables to a model. You can compare models with a likelihood ratio test or an F test. Anova facilitates comparison of simple regression models.

```{r}
# Add variable wt
reg_1 <- lm( mpg ~ hp + cyl + am + wt, mtcars)

# Aikikae Information Criteria
# AIC lower the better
AIC(reg_0)
AIC(reg_1)

# Compare
anova(reg_0, reg_1) # models are significantly different
```

### Logistic Regression

Let us change gears and try to predict a binary variable. For this purpose we will use the logistic regression with a binomial link function. The model estimates the probability of Y=1.

Let us stick to the mtcars dataset and try to figure out if a car is automatic or manual based on predictors.

We will use glm function.

```{r}
?glm
```

Let us fit the model

```{r}
logit_2 <- glm(am ~ mpg + drat + cyl, data = mtcars, family='binomial')
summary(logit_2)
```

Visualize the results.

```{r}
ggplot(mtcars, aes(x = mpg, y = am)) + 
    stat_smooth(method="glm", family="binomial", se=FALSE)+
# Bonus: rename the y axis label
		ylab('Probability of Manual Transmission')
```

How about plotting results for number of cylinders? We will need to process the data a little bit.

```{r}
# Create a new dataset with varying number of cylinders and other variables fixed at mean levels.
mtcars2<-data.frame(mpg = rep(10:30, 3),drat = mean(mtcars$drat), disp = mean(mtcars$disp), cyl = rep(c(4,6,8),21))
# Predict probability of new data
mtcars2$prob<-predict(logit_2, newdata=mtcars2, type = "response")

# Plot the results
ggplot(mtcars2, aes(x=mpg, y=prob)) +
  geom_line(aes(colour = factor(cyl)), size = 1) 
```

Diagnostics with logistic regression.

```{r}
library(caret)

# Let us compare predicted values to real values
mtcars$prob <- predict(logit_2, type="response")
# Prevalence of Manual Transmission
mean(mtcars$am)

# Create predict variable
mtcars$pred <- 0
# If probability is greater than .6 (1-prevalence), set prediction to 1
mtcars[mtcars$prob>.6, 'pred'] <- 1

# Confusion Matrix
confusionMatrix(table(mtcars[,c("am", "pred")]))

## ROC CURVE
# Load the necessary library
library(pROC)
# Calculate the ROC curve using the predicted probability vs actual values
logit_2_roc <- roc(am~prob, mtcars) 
# Plot ROC curve
plot(logit_2_roc)
```

## Caret Package

Caret package is a wrapper that brings together functionality from 27 packages. Caret supports [estimating over 150 models](http://topepo.github.io/caret/bytag.html) including bayesian, SVM, discriminant analysis, regressions, neural networks, and more.

```{r, eval=FALSE}
# Run below commands to get a list of related packages
available.packages()["caret","Depends"]
available.packages()["caret","Suggests"]
```

Caret package aims to be the go to package for your predictive analytics needs. Thus it not only covers model training, but also data manipulation, visualization, and parallelization capabilities.

Since so many packages involved, the installation takes a while.

```{r ,eval=FALSE}
install.packages("caret", dependencies = c("Depends", "Suggests"))
```

The main idea is comparing performance of alternate models and finding the best fit. Clarification of ambiguities: 
 1. Alternate models: test various model parameters
 2. Best fit: according to various metrics
 3. Compare performance: through resampling.

## Model Training - Classification Trees with caret Package

In this section, I will be presenting a classification tree with caret package. The example presented here follows closely [Kuhn's UseR 2013 presentation](https://www.r-project.org/nosvn/conferences/useR-2013/Tutorials/kuhn/user_caret_2up.pdf). I had to leave out significant portions of Kuhn's work to fit it into the scope of our workshop. Please refer to the original material for details.


### Data

We will use segmentationData from the caret package.

```{r}
library(caret) # Load necessary library
data(segmentationData) # Obtain Dataset
```

We do not need the Cell identifiers.

```{r}
# Drop one column
segmentationData <- segmentationData[,!(colnames(segmentationData)=='Cell')] 
```

The data has a variable "Case" to indicate Training, vs Testing observations. We will obtain subsets based on this variable.

```{r}
# Data set has a variable that separates training vs testing
Training <- segmentationData[segmentationData$Case == 'Train', ] #$ One Way
Testing <- subset(segmentationData, Case == 'Test') # Another way
# Drop the now defunct Case variable
Training <- Training[,!(colnames(Training)=='Case')] # Drop Case column
Testing <- subset(Testing, select=-c(Case)) # Another way
```

### Classification Trees without Caret

I want to demonstrate how Classification Trees can be fit without caret first, so that the contribution of caret's train function becomes clearer.

Let us first fit a tree of a limited depth.

```{r, fig.height=6}
library(rpart) # Load necessary library
rpart_1 <- rpart(Class ~ ., data = Training, 
  			control = rpart.control(maxdepth=2)) # Fit a shallow tree 
rpart_1 # View results

plot(rpart_1) # Visualization
text(rpart_1) # Text
```

When you fit a tree with rpart, it conducts as many spits as possible, then use 10 fold cross validation to prune the tree.

```{r}
# Fit a larger tree and prune it 
# Rpart does 10 fold cross validation
rpart_2 <- rpart(Class ~ ., data = Training) 
```

partykit package provides beter plots for classification trees. 

```{r, fig.height=8, fig.width=10}
library(partykit)
plot(as.party(rpart_2))
```

```{r}
# Validate the model
rpart_2Test <- predict(rpart_2, Testing, type='class')
confusionMatrix(rpart_2Test, Testing$Class) # Get the benchmark$
```


## Model Tuning - Classification Trees with Caret

caret package allows you to change tuning parameters and resampling strategies for the models. Below we instruct caret to use ROC curve with k-fold cross validation repeated 3 times to pick the best model.

```{r}
library(caret)
##  Set Training Parameters
# Triple Cross Validation
cvCtrl <- trainControl(method = "repeatedcv", repeats = 3, # K-fold cross validation repeated thrice
  				summaryFunction = twoClassSummary, classProbs = TRUE) # Class probabilities for ROC

# Use caret to fit a model and fine tune the fit
CarrotTree <- train(Class ~., data=Training, method='rpart', 
					trControl=cvCtrl, metric = "ROC", tuneLength=30) # Change metric to ROC
```

Let us see the performance of models over various levels of tuning parameter.

```{r}
plot(CarrotTree)
```

Let us see the final model

```{r, fig.height=8, fig.width=10, eval=FALSE}
# Output omited to conserve space
plot(as.party(CarrotTree$finalModel)) 
```


### Validate Using Testing Data

Predict the class membership with test data and then plot out the confusion matrix for performance metrics of this model over test data.

```{r}
# Testing
CarrotTree_Test <- predict(CarrotTree, Testing)
head(CarrotTree_Test) # Compare
head(Testing$Class) # and Contrast
# Evaluate Model Performance
confusionMatrix(CarrotTree_Test, Testing$Class) 
```


## Model Training - Discriminant Analysis with caret Package

In this section I will present a classification example using caret package. The example presented here follows closely [the caret package vignette](https://cran.r-project.org/web/packages/caret/vignettes/caret.pdf). I only made a few changes to fit it into the workshop's time frame. So if you need any clarification, you can read up more in the vignette.


### Data

We will use the sonar dataset from mlbench package. It has 208 observations of 60 predictor variables and a binary class variable as dependent. We do not know what these 60 variables are.

```{r}
# Dataset comes with mlbench package
library(mlbench)
# Load dataset into the current workspace
data(Sonar)
```

### SPLIT THE DATA

This dataset is not conveniently split as the previous set was, we need to create our own subsets. Caret provides functionality to split the data into a training and a testing set while at the same time preserving the distribution of dependent variable.

createDataPartition function receives the dependent variable, proportion of training set in the whole of dataset as parameters.

```{r}
library(caret) # Load Library
set.seed(107)  # Set random number seed for reproducibility
# Create an index of observations to be included in Training
indexTrain <- createDataPartition(y = Sonar$Class, p = .75, list = FALSE)

# Split the data using the index
Train <- Sonar[indexTrain,]
Test  <- Sonar[-indexTrain,]
```

### Training a Discriminant Model

We will be using a PLS DA model to train.

First step is to set resampling and validation strategy. Here we are using 3 resamplings of 10-fold cross validation. We also configure the trainer to produce predicted probabilities to be used in ROC calculation.

```{r}
## Declare Tuning Control Parameters
ctrl <- trainControl(method = "repeatedcv", # K-fold cross validation
			        repeats = 3, # Repeat resampling 3 times
                    classProbs = TRUE, # Calculate predicted prob for ROC
                    summaryFunction = twoClassSummary) # Set performance metrics for binary
```

Below we train the model with varying parameters. The tune length in this case specifies maximum the number of components to be extracted.

```{r}
plsFit <- train(Class ~ ., data = Train, method = "pls",
		tuneLength = 10,   # Number of component sets to be evaluated (more is better)
		trControl = ctrl, # Use control parameters from above
		metric = "ROC" ,  # Criteria ROC
		preProc = c("center", "scale")) # Center and scale the predictors
```

Let us review the models that were fit.

```{r}
plsFit
```

As you can see the area under the ROC curve increases up till 4 components and starts declining afterwards. By default the model with 4 components is used.

Plotting this fit would demonstrate the change in area under ROC for different components.

```{r}
# evaluate the performance of different number of components extracted
plot(plsFit)
```

### Validate with Test Data

Let us see how our fitted model performs with Test data.

```{r}
plsPredict <- predict(plsFit, newdata = Test) # Predict results
head(plsPredict) # View predictions
head(Test$Class) # View actual$
```

Get confusion matrix (predicted vs actual) for performance reports.

```{r}
confusionMatrix(data = plsPredict, Test$Class)
```

